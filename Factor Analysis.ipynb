{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9001e9e0",
   "metadata": {},
   "source": [
    "# Factor analysis on chemical abundances spaces\n",
    "\n",
    "Authors: Yangda Bei, Yuan-Sen Ting\n",
    "\n",
    "The following notebook acts as an interactive companion to research project (ASC) completed during the Summer session 2022 and contains the code for the latent factor model created for the ASC. Although such a model exists within the ``FactorAnalyzer`` package created by ``scikit-learn``, a factor model was created from scratch following ``Algorithm 21.1`` from Barber's *Bayesian Reasoning and Machine Learning* as an exercise in data analysis using Python and introduction to machine learning models. The notebook provides better readability of the individual steps performed using the algorithm.\n",
    "\n",
    "Before we begin, let's import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76753145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from cycler import cycler\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "# define plot properties\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def rgb(r,g,b):\n",
    "    return (float(r)/256.,float(g)/256.,float(b)/256.)\n",
    "\n",
    "cb2 = [rgb(0,0,0), rgb(255,127,0), rgb(31,120,180), rgb(51,160,44), rgb(227,26,28), \\\n",
    "       rgb(166,206,227), rgb(253,191,111), rgb(178,223,138), rgb(251,154,153)]\n",
    "\n",
    "rcParams['figure.figsize'] = (9,8)\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "rcParams['lines.linewidth'] = 1\n",
    "\n",
    "rcParams['axes.prop_cycle'] = cycler('color', cb2)\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['axes.grid'] = False\n",
    "\n",
    "rcParams['patch.facecolor'] = cb2[0]\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "\n",
    "rcParams['font.size'] = 10\n",
    "rcParams['font.weight'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb534af",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc89939",
   "metadata": {},
   "source": [
    "## 1. Creating mock factors and dataset\n",
    "\n",
    "First, we set our latent factor vectors, $\\mathbf{v}\\in\\mathbb{R}^{H\\times D}$. We wish to compare how well these latent factors can be recovered by using the ``FactorAnalyzer`` package and by using our own factor model. The data used will be drawn from ``latentfactors.csv``. It consists of $H=5$ latent factors with $D=25$ dimensions drawn from random for the sake of consistency however there is the option to generate random factors with different values for ``num_latent`` and ``num_dimension``.\n",
    "\n",
    "Run the first code block for randomly generated latent factors.\n",
    "\n",
    "Run the second code block to get latent factors from ``latentfactors.csv``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201e3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# run for randomly generated latent factors\n",
    "#----------------------------------------------------\n",
    "\n",
    "# number of latent factors\n",
    "num_latent = 5\n",
    "\n",
    "# number of observed dimensions\n",
    "num_dimension = 25\n",
    "\n",
    "# make latent vectors\n",
    "v = np.random.normal(size=(num_latent, num_dimension))\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a27161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# run for latent factors used in report\n",
    "#-------------------------------------------------------\n",
    "\n",
    "num_latent = 5\n",
    "\n",
    "num_dimension = 25\n",
    "\n",
    "with open(\"latentfactors.csv\") as latent_factors:\n",
    "    v = np.loadtxt(\"latentfactors.csv\", delimiter = \",\")\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58671168",
   "metadata": {},
   "source": [
    "Next, we create a mock dataset with $D$ dimensions (features) and $N$ samples of random values drawn from a Gaussian. To do so, we first create our coefficients for our factor loading matrix $\\mathbf{\\alpha}\\in\\mathbb{R}^{N\\times H}$, then multiply with the $\\mathbf{v}$ factors. \n",
    "\n",
    "The mock dataset used will be drawn from ``mockdata.csv`` with $D=25$ and $N=10\\,000$. Note that for samples of order >$10^4$, memory allocation will become an issue. Future improvements include improving scalability of SVD algorithm (``dask.array.linalg.svd`` is a promising solution).\n",
    "\n",
    "Run the first code block for randomly generated samples.\n",
    "\n",
    "Run the second code block to get dataset from ``mockdata.csv``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ea567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# run for randomly generated samples\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# number of mock samples\n",
    "num_sample = 10**4\n",
    "\n",
    "# make the factor loading matrix (N x H)\n",
    "alpha = np.random.normal(size=(num_sample, num_latent))\n",
    "\n",
    "# make mock samples (N x D)\n",
    "V = np.dot(alpha,v)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# run for dataset used in analysis in report\n",
    "#-------------------------------------------------------\n",
    "\n",
    "num_sample = 10**4\n",
    "\n",
    "with open(\"mockdata.csv\") as mock_data:\n",
    "    V = np.loadtxt(\"mockdata.csv\", delimiter = \",\")\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c33af",
   "metadata": {},
   "source": [
    "Now that we have made our dataset with underlying factors, we can visualise the correlation between the $D$ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the dataset into a Pandas dataframe for convenience in\n",
    "# creating heatmap\n",
    "data = pd.DataFrame(V)\n",
    "data.rename(columns={i:i+1 for i in range(num_dimension)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates correlation heatmap\n",
    "plt.title(\"Correlation Heatmap\", fontsize = 10)\n",
    "\n",
    "plt.imshow(data.corr(), cmap = \"cividis\", interpolation = \"none\")\n",
    "plt.xticks(range(len(data.columns)), data.columns)\n",
    "plt.yticks(range(len(data.columns)), data.columns)\n",
    "\n",
    "plt.gcf().set_size_inches(6,6)\n",
    "plt.tick_params(labelsize=7)\n",
    "cbar = plt.colorbar(shrink = 0.7)\n",
    "cbar.ax.tick_params(labelsize=7)\n",
    "\n",
    "# show correlation value of each square\n",
    "labels = data.corr().values\n",
    "for y in range(labels.shape[0]):\n",
    "    for x in range(labels.shape[1]):\n",
    "        plt.text(x,y, \"{:.2f}\".format(labels[y,x]), ha = \"center\", \n",
    "                 va=\"center\", color = \"white\", fontsize = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb6170",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fe9e5",
   "metadata": {},
   "source": [
    "## 2. Factor analysis using ``FactorAnalyzer`` by ``scikit-learn``\n",
    "\n",
    "The following code blocks generate an exploratory factor observation of the underlying latent factors of $\\mathbf{V}$ using the ``FactorAnalyzer`` package after specifying the number of components wished to be extracted. \n",
    "\n",
    "First, we must add some noise in order to properly define the likelihood of our data. We map two dimensions of the mockdata after noise has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe875b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add noise \n",
    "V_noise = V+0.01*np.random.normal(size=V.shape)\n",
    "\n",
    "# show two of the dimensions\n",
    "plt.hexbin(V_noise[:,0], V_noise[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4fe2f",
   "metadata": {},
   "source": [
    "For *confirmatory* factor analysis, we already know the number of latent factors and are just *confirming* that they are there in the dataset. For *exploratory* factor analysis on the other hand, we need to determine the number of compenents that are hidden before we can call ``.fit_transform()``. Since we made up the number of latent factors, we will just determine the number of factors as a check.\n",
    "\n",
    "To find out how many factors we can extract, we look to the eigenvalues of the correlation matrix. These eigenvalues tell us how much variance of the variables a factor explains. Therefore, an eigenvalue that is $>1$ means the factor that corresponds to it explains the variance of more than one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert correlation DataFrame made in section 1 to an\n",
    "# ndarray\n",
    "correlation = data.corr()\n",
    "correlation.to_numpy\n",
    "\n",
    "# extract eigenvalues and sort them in descending order\n",
    "evalues,_ = np.linalg.eig(correlation)\n",
    "evalues[::-1].sort()\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# scree plot\n",
    "plt.plot(range(1, correlation.shape[1] + 1),evalues, 'o-')\n",
    "\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.xlabel(\"Factor\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14800f86",
   "metadata": {},
   "source": [
    "We can see that the number of factors matches the value of ``num_latent``! Now, we can call ``.fit_transform()`` on the dataset to transform all the features using its mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform latent decomposition using sklearn package\n",
    "fa = FactorAnalysis(n_components=num_latent, noise_variance_init=0.01*np.ones(num_dimension))\n",
    "v_transformed = fa.fit_transform(V_noise)\n",
    "\n",
    "print(v_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af56e2",
   "metadata": {},
   "source": [
    "The factors are then extracted using ``.components`` and reordered by how close they are to the true latent factors. The components are then compared to the true latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b1cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracts components\n",
    "fa_components = fa.components_\n",
    "print(fa_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1deef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorders the latent vectors\n",
    "ind_best = np.zeros(num_latent)\n",
    "\n",
    "for i in range(num_latent):\n",
    "    ind_temp = -999\n",
    "    diff_temp = np.inf\n",
    "    \n",
    "    for j in range(num_latent):\n",
    "        diff = np.sum((v[i,:] - fa_components[j,:])**2)\n",
    "        if diff < diff_temp:\n",
    "            diff_temp = diff\n",
    "            ind_temp = j\n",
    "            \n",
    "    ind_best[i] = ind_temp\n",
    "\n",
    "print(ind_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd25df8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creates plot comparing true latent factors and package fit\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, sharex=True, sharey=True, figsize=(10, 6))\n",
    "\n",
    "fig.text(0.5, 0.06, r\"Index of $v_i$\", ha='center')\n",
    "fig.text(0.06, 0.5, r\"Value of $v_i$\", va='center', rotation='vertical')\n",
    "\n",
    "fig.suptitle(\"Comparison of true factors and package fit\", y = 0.92)\n",
    "\n",
    "# creates subplots\n",
    "for i in range(num_latent):\n",
    "\n",
    "    axs[i].plot(v[i,:], label = \"Latent factor\", ls = \"--\", )\n",
    "    axs[i].plot(fa_components[int(ind_best[i]),:], label = r\"$\\mathtt{FactorAnalyzer}$ fit\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 2.9), prop={'size': 8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c50ac",
   "metadata": {},
   "source": [
    "Note that some factors may be deemed to be the best fit for more than one extracted component.\n",
    "\n",
    "Lastly, the noise estimated by ``FactorAnalyzer`` is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated noise\n",
    "print(fa.noise_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a159b14",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0451f",
   "metadata": {},
   "source": [
    "## 3. Factor analysis from scratch\n",
    "\n",
    "Next, we build our factor model based on ``Algorithm 21.1`` from Barber's *Bayesian Reasoning and Machine Learning* (2020).\n",
    "\n",
    "The factor analysis model generates an observation $\\mathbf{v}$ of a given dataset $\\mathbf{V} = \\{\\mathbf{v}^1,\\dots,\\mathbf{v}^N\\}$ according to \n",
    "\n",
    "$$\\mathbf{V} = \\mathbf{Fh} + \\mathbf{M} + \\mathbf{\\varepsilon}.$$\n",
    "\n",
    "Here, we use slightly different notation from Section 2. $\\mathbf{V}\\in \\mathbb{R}^{D\\times N}$ is our output observation. $\\mathbf{F}\\in\\mathbb{R}^{D\\times H}$ is our factor matrix that contains the \"weights\" of each factor. $\\mathbf{h}\\in\\mathbb{R}^{H\\times N}$ is our factor loading matrix. $\\mathbf{M}\\in \\mathbb{R}^{D\\times N}$ is the constant bias which is the mean in this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7405daf",
   "metadata": {},
   "source": [
    "**Algorithm 21.1**\n",
    "\n",
    "> 1. Initialise diagonal noise $\\mathbf{\\Psi}$.\n",
    "\n",
    "We initialise our noise by setting $\\mathbf{\\Psi}=\\text{diag}(\\psi_1,\\dots,\\psi_D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialise diagonal noise\n",
    "noise = 0.01*np.random.normal(size=(num_dimension,num_dimension))\n",
    "psi = np.diag(np.diag(np.cov(noise)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327d7a3",
   "metadata": {},
   "source": [
    "> 2. Find the mean $\\mathbf{\\bar v}$ of the data $\\mathbf{v}^1,\\dots, \\mathbf{v}^N$.\n",
    "\n",
    "This is our constant bias that we use to \"shift\" our coordinate system to better fit the data. The mean matrix is set to be ``M`` in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f58d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Find the mean of the data\n",
    "result = []\n",
    "for vector in V:\n",
    "    result.append([sum(vector)/len(vector)])\n",
    "\n",
    "M = np.array(result)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6becb",
   "metadata": {},
   "source": [
    "> 3. Find the variance $\\sigma_i^2$ for each component $i$ of the data $v_i^1,\\dots, v_i^N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4751778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Find the variance\n",
    "var = np.var(V, axis=0)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca39108",
   "metadata": {},
   "source": [
    "> 4. Compute the centred matrix $\\mathbf{X} = \\mathbf{V} - \\mathbf{M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d07721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute the centred matrix\n",
    "X = V - M\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce10bd",
   "metadata": {},
   "source": [
    "> 5. **while** Likelihood not converged or termination criterion not reached **do**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd569e7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initial constants and variables\n",
    "L_old = -np.infty\n",
    "likelihoods = []\n",
    "    \n",
    "# error tolerance\n",
    "tol = 1e-4\n",
    "\n",
    "# max iterations before termination\n",
    "max_iter = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50280a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. While loop runs if error is greater than tolerance or until max_iter\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    # 6. Form the scaled data matrix\n",
    "    X_tilde = np.dot(np.linalg.inv(np.sqrt(psi)), X.T / (num_sample ** 0.5))\n",
    "    #----------------------------------------------------------------------------\n",
    "    # 7. Perform SVD for the scaled data matrix\n",
    "    U,Lambda_tilde,WT = np.linalg.svd(X_tilde)\n",
    "    Lambda_tilde = np.diag(Lambda_tilde)\n",
    "    Lambda = Lambda_tilde ** 2\n",
    "    #----------------------------------------------------------------------------\n",
    "    # 8. Set U_H to the first H columns of U and set Lambda_H to contain the \n",
    "    #    first H diagonal entries of Lambda\n",
    "    H = num_latent\n",
    "    U_H = U[:,:H]\n",
    "    Lambda_H = Lambda_tilde[:H,:H]\n",
    "    #----------------------------------------------------------------------------\n",
    "    # 9. Factor update\n",
    "    #if i == 0:\n",
    "        #F = np.copy(v.T)\n",
    "    #else:\n",
    "    F = np.sqrt(psi) @ U_H @ np.sqrt(Lambda_H - np.identity(H))\n",
    "    #----------------------------------------------------------------------------\n",
    "    # 10. Log likelihood\n",
    "    a = 0\n",
    "    for j in range(H):\n",
    "        a += np.log(Lambda[j,j])\n",
    "    b = 0\n",
    "    for k in range(H+1,num_dimension):\n",
    "        b += Lambda[k,k]\n",
    "    \n",
    "    L_new = (num_dimension/2)*(a + H + b + np.log(np.linalg.det(2*np.pi*psi)))\n",
    "    likelihoods.append(L_new)\n",
    "    #----------------------------------------------------------------------------\n",
    "    # 11. Psi update\n",
    "    psi = np.maximum(np.diag(var)-np.diag(F@F.T),1e-12)\n",
    "    #----------------------------------------------------------------------------\n",
    "    print(\"Iteration: %d\" % (i+1))\n",
    "    print(\"Loglikehood: \", L_new)\n",
    "        \n",
    "    if np.abs(L_new-L_old)<tol:\n",
    "        break\n",
    "    \n",
    "    # log likelihood update\n",
    "    L_old = L_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95af270",
   "metadata": {},
   "source": [
    "> 12. **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dc682",
   "metadata": {},
   "source": [
    "The following plot shows how well the model fits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a6937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates plot comparing true latent factors and model fit\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, sharex=True, sharey=True, figsize=(10, 6))\n",
    "\n",
    "fig.text(0.5, 0.06, r\"Index of $v_i$\", ha='center')\n",
    "fig.text(0.06, 0.5, r\"Value of $v_i$\", va='center', rotation='vertical')\n",
    "\n",
    "fig.suptitle(\"Comparison of true factors and model fit\", y = 0.92)\n",
    "\n",
    "for i in range(num_latent):\n",
    "\n",
    "    # creates subplots\n",
    "    axs[i].plot(v[i,:], label = \"Latent factor\", ls = \"--\", )\n",
    "    axs[i].plot(F.T[int(ind_best[i]),:], label = \"Model fit\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 2.9), prop={'size': 8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c3650",
   "metadata": {},
   "source": [
    "The following plot shows the comparison between ``FactorAnalyzer`` and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates plot comparing package fit and model fit\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, sharex=True, sharey=True, figsize=(10, 6))\n",
    "\n",
    "fig.text(0.5, 0.06, r\"Index of $v_i$\", ha='center')\n",
    "fig.text(0.06, 0.5, r\"Value of $v_i$\", va='center', rotation='vertical')\n",
    "\n",
    "fig.suptitle(\"Comparison of package fit and model fit\", y = 0.92)\n",
    "\n",
    "# creates subplots\n",
    "for i in range(num_latent):\n",
    "\n",
    "    axs[i].plot(fa_components[int(ind_best[i]),:], label = r\"$\\mathtt{FactorAnalyzer}$ fit\")\n",
    "    axs[i].plot(F.T[i,:], label = \"Model fit\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 2.9), prop={'size': 8})\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
